{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f376d1a3-1658-417c-96c7-0b2e3813392a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "Epoch 1/5\n",
      "200/200 - 4s - 18ms/step - accuracy: 0.6708 - loss: 0.5981 - val_accuracy: 0.7584 - val_loss: 0.5046\n",
      "Epoch 2/5\n",
      "200/200 - 3s - 15ms/step - accuracy: 0.8367 - loss: 0.3834 - val_accuracy: 0.8292 - val_loss: 0.4042\n",
      "Epoch 3/5\n",
      "200/200 - 3s - 15ms/step - accuracy: 0.9023 - loss: 0.2565 - val_accuracy: 0.8132 - val_loss: 0.4196\n",
      "Epoch 4/5\n",
      "200/200 - 3s - 15ms/step - accuracy: 0.9514 - loss: 0.1544 - val_accuracy: 0.8006 - val_loss: 0.4589\n",
      "Epoch 5/5\n",
      "200/200 - 3s - 16ms/step - accuracy: 0.9748 - loss: 0.0941 - val_accuracy: 0.8170 - val_loss: 0.4914\n",
      "\n",
      "테스트 Loss: 0.50722, 테스트 정확도: 81.016%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 텐서플로우의 로그 레벨을 설정하여 경고 메시지를 숨깁니다.\n",
    "\n",
    "import tensorflow as tf  # 텐서플로우 라이브러리를 임포트합니다.\n",
    "from tensorflow.keras import layers, Sequential  # 케라스에서 필요한 레이어와 Sequential 모델을 임포트합니다.\n",
    "from tensorflow.keras.optimizers import Adam  # Adam 옵티마이저를 임포트합니다.\n",
    "from tensorflow.keras.datasets import imdb  # IMDb 데이터셋을 임포트합니다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # 패딩 함수를 임포트합니다.\n",
    "\n",
    "# 데이터셋을 로드하고 전처리하는 함수입니다.\n",
    "def load_data(num_words, max_len):\n",
    "    # imdb 데이터셋을 불러옵니다. 데이터셋에서 단어는 num_words 개를 가져옵니다.\n",
    "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "    # 단어 개수가 다른 문장들을 Padding을 추가하여\n",
    "    # 단어가 가장 많은 문장의 단어 개수로 통일합니다.\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)  # X_train 데이터에 대해 패딩을 적용합니다.\n",
    "    X_test = pad_sequences(X_test, maxlen=max_len)  # X_test 데이터에 대해 패딩을 적용합니다.\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test  # 전처리된 데이터셋을 반환합니다.\n",
    "\n",
    "# RNN 모델을 정의하는 함수입니다.\n",
    "def build_rnn_model(num_words, embedding_len):\n",
    "    model = Sequential()  # Sequential 모델을 초기화합니다.\n",
    "    \n",
    "    # 임베딩 레이어를 추가합니다.\n",
    "    # input_dim: 단어 집합의 크기 (여기서는 num_words)\n",
    "    # output_dim: 임베딩 벡터의 차원 (여기서는 embedding_len)\n",
    "    model.add(layers.Embedding(input_dim=num_words, output_dim=embedding_len))\n",
    "    \n",
    "    # SimpleRNN 레이어를 추가합니다.\n",
    "    # units: RNN의 유닛 수 (여기서는 16)\n",
    "    model.add(layers.SimpleRNN(16))\n",
    "    \n",
    "    # Dense 레이어를 추가합니다.\n",
    "    # units: 출력 유닛 수 (여기서는 1)\n",
    "    # activation: 활성화 함수 (여기서는 'sigmoid' 사용)\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model  # 모델을 반환합니다.\n",
    "\n",
    "# 메인 함수입니다.\n",
    "def main(model=None, epochs=5):\n",
    "    # IMDb 데이터셋에서 가져올 단어의 개수\n",
    "    num_words = 6000\n",
    "    \n",
    "    # 각 문장이 가질 수 있는 최대 단어 개수\n",
    "    max_len = 130\n",
    "    \n",
    "    # 임베딩 된 벡터의 길이\n",
    "    embedding_len = 100\n",
    "    \n",
    "    # IMDb 데이터셋을 불러옵니다.\n",
    "    X_train, X_test, y_train, y_test = load_data(num_words, max_len)\n",
    "    \n",
    "    if model is None:\n",
    "        model = build_rnn_model(num_words, embedding_len)\n",
    "    \n",
    "    # 모델 학습을 위한 optimizer와 loss 함수를 설정합니다.\n",
    "    # optimizer: Adam 옵티마이저를 사용하고, 학습률은 0.001로 설정합니다.\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    \n",
    "    # 모델을 컴파일합니다.\n",
    "    # loss: 이진 교차 엔트로피 손실 함수를 사용합니다.\n",
    "    # metrics: 평가 지표로 정확도를 사용합니다.\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # 모델을 학습합니다.\n",
    "    # epochs: 전체 데이터셋을 몇 번 반복할지 설정합니다.\n",
    "    # batch_size: 한 번에 몇 개의 샘플을 학습할지 설정합니다.\n",
    "    # validation_split: 검증을 위해 학습 데이터의 몇 %를 사용할지 설정합니다.\n",
    "    # shuffle: 데이터를 학습 전에 셔플할지 설정합니다.\n",
    "    # verbose: 학습 중 출력할 로그의 레벨을 설정합니다.\n",
    "    hist = model.fit(X_train, y_train, epochs=epochs, batch_size=100, validation_split=0.2, shuffle=True, verbose=2)\n",
    "    \n",
    "    # 모델을 테스트 데이터셋으로 테스트합니다.\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print()\n",
    "    print(\"테스트 Loss: {:.5f}, 테스트 정확도: {:.3f}%\".format(test_loss, test_acc * 100))\n",
    "    \n",
    "    return optimizer, hist  # 옵티마이저와 학습 기록을 반환합니다.\n",
    "\n",
    "# 스크립트가 메인 프로그램으로 실행될 때 main() 함수를 호출합니다.\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77197b-9a5d-4fc1-b62f-acfd44795100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
